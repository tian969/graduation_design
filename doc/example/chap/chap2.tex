% Copyright (c) 2014,2016 Casper Ti. Vector
% Public domain.

\chapter{研究进展}
\section{通用大模型面临的核心风险}
近年来，大语言模型（LLM）的飞速发展显著提升了自然语言处理的基准，并进一步催生了视觉语言模型（VLM）的多模态理解能力。然而，随着模型参数量和应用场景的扩张，其潜藏的安全风险也愈发凸显。
正常场景中, 面临的多模安全风险主要可以分为以下几类:
文本风险, 图片风险, 图文联合风险.
安全的难点在于
1. 安全任务和其他任务是背道而驰的. 安全的目标是不输出风险内容, 而模型内生的指令遵循能力又要求模型尽可能的按照指令进行输出, 因此强化安全能力势必削弱通用能力.
2. 边界的不确定性, 各类主体针对安全边界的认知存在明显差异. 因此, 很多第三方的安全边界是相对缺乏参考价值的.
现有的安全性研究主要集中在以下几个核心挑战：
- 过度拒绝与拒绝不足： 模型在安全性和有用性之间存在帕累托前沿（Pareto frontier）的博弈。这导致模型要么容易被欺骗输出有害内容，要么过度保守，拒绝回答合法的良性请求（Overrefusal）。
- 越狱攻击： 恶意用户可以通过精心设计的提示词（Adversarial Prompts）诱导模型绕过内部安全准则，生成不当或有害内容，如非法行为指令、自残建议等。
- 幻觉问题: 模型生成事实性错误或逻辑不连贯内容的现象，在视觉语言模型中尤为严重。
- 种族歧视与偏见: 模型在海量互联网数据预训练过程中，不可避免地吸收了人类社会存在的社会偏见。这种偏见不仅体现在文本回复中，更在图像识别与关联中被放大。例如，模型可能在视觉识别中对特定族裔产生刻板印象，或在招聘、风险评估等下游任务中表现出系统性的种族或性别歧视。这不仅是技术挑战，更是违反 AI 伦理准则的核心问题，直接影响了模型的大规模部署与社会合规性。
我们的研究主要聚焦于提高输出安全, 应对拒绝不足和越狱攻击, 以及通过更合理地利用外挂知识库, 解决可能出现的幻觉问题.

\section{推理与部署阶段的安全增强方法}
\subsection{输入输出安全审核与后处理}
通过构建完整的安全审查链路以阻断不良信息的正常流转，是当前输入输出安全防护领域的主流思路，亦是工业界应用最为广泛的技术方案之一。
它的相关研究成果颇丰，其核心设计理念在于引入一个参数量更小的轻量模型，专门负责对输入内容与输出结果进行安全校验。
该校验过程可灵活作用于两个环节：既可以对用户发起的查询请求进行前置筛查，也可在主模型完成推理生成后，对输出内容进行后置校验。典型的代表性模型包括Llama-Guard、Llava-Guard、Shield-Gemma、Qwen3-Guard、WildGuard等。具体而言，在主模型完成推理任务后，由该轻量化校验模型评判输出结果是否违反预设安全准则，若检测到违规内容，则触发对应的后处理流程，相关后处理方案可参考VLMGuard-R1、MLLM-Protector等研究。
此类方法实现逻辑简洁易懂，与主模型的训练及推理流程完全解耦隔离，便于研究人员与工程师根据实际需求定制化设计安全策略，具备较强的工程实用性。同时，其局限性亦不容忽视：安全防护能力的生效依赖于完整审查链路的部署，若直接对开源模型发起查询，模型仍可能接收有害输入并生成违规输出。此外，采用前置分类模型进行校验时，模型的泛化能力存在明显瓶颈，当安全规则需要频繁调整时，需对前置分类模型进行反复训练与迭代，维护成本较高。


% vim:ts=4:sw=4
